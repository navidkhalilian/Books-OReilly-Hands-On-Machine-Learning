# فهرست مطالب

## پیش‌گفتار
- سونامی یادگیری ماشین  
- یادگیری ماشین در پروژه‌های شما  
- هدف و رویکرد  
- نمونه‌کدها  
- پیش‌نیازها  
- نقشه راه  
- تغییرات بین نسخه اول و دوم  
- تغییرات بین نسخه دوم و سوم  
- منابع دیگر  
- قراردادهای نگارشی مورد استفاده در این کتاب  
- یادگیری آنلاین اورایلی (O’Reilly)  
- راه‌های ارتباط با ما  
- تقدیر و تشکر  

---

## بخش اول: مبانی یادگیری ماشین

### فصل ۱: چشم‌انداز یادگیری ماشین
- یادگیری ماشین چیست؟  
- چرا باید از یادگیری ماشین استفاده کنیم؟  
- نمونه‌هایی از کاربردها  
- انواع سیستم‌های یادگیری ماشین  
- نظارت در فرایند آموزش  
- یادگیری دسته‌ای در برابر یادگیری آنلاین  
- یادگیری مبتنی بر نمونه در برابر یادگیری مبتنی بر مدل  
- چالش‌های اصلی در یادگیری ماشین  
  - کمبود داده‌های آموزشی  
  - داده‌های آموزشی نامناسب و غیرنماینده  
  - داده‌های بی‌کیفیت  
  - ویژگی‌های نامرتبط  
  - بیش‌برازش به داده‌های آموزشی  
  - کم‌برازش به داده‌های آموزشی  
- یک گام به عقب: بازنگری مفهومی  
- آزمون و ارزیابی مدل  
- تنظیم ابرپارامترها و انتخاب مدل  
- ناسازگاری داده‌ها  
- تمرین‌ها  

## فصل ۲: پروژه‌ی یادگیری ماشین از ابتدا تا انتها

- کار با داده‌های واقعی  
- نگاهی کلی به تصویر بزرگ  
- تعریف دقیق مسئله  
- انتخاب معیار ارزیابی عملکرد  
- بررسی فرضیات اولیه  
- دریافت داده‌ها  
- اجرای نمونه‌کدها با استفاده از Google Colab  
- ذخیره تغییرات کد و داده‌ها  
- قدرت و ریسک تعامل‌پذیری  
- تفاوت کدهای کتاب و کدهای نوت‌بوک  
- دانلود داده‌ها  
- بررسی اولیه‌ی ساختار داده‌ها  
- ایجاد مجموعه‌ی تست  
- کاوش و مصورسازی داده‌ها برای استخراج بینش  
- مصورسازی داده‌های جغرافیایی  
- بررسی همبستگی‌ها  
- آزمایش ترکیب ویژگی‌ها  
- آماده‌سازی داده‌ها برای الگوریتم‌های یادگیری ماشین  
  - پاک‌سازی داده‌ها  
  - مدیریت ویژگی‌های متنی و دسته‌ای  
  - مقیاس‌بندی و تبدیل ویژگی‌ها  
  - تبدیل‌کننده‌های سفارشی  
  - خط لوله‌های تبدیل (Pipelines)  
- انتخاب و آموزش مدل  
  - آموزش و ارزیابی روی مجموعه آموزش  
  - ارزیابی بهتر با استفاده از اعتبارسنجی متقاطع  
- تنظیم دقیق مدل  
  - جستجوی شبکه‌ای (Grid Search)  
  - جستجوی تصادفی (Randomized Search)  
  - روش‌های تجمیعی (Ensemble Methods)  
- تحلیل بهترین مدل‌ها و خطاهای آن‌ها  
- ارزیابی نهایی روی مجموعه‌ی تست  
- راه‌اندازی، پایش و نگهداری سیستم  
- خودتان امتحان کنید!  
- تمرین‌ها  

---

## فصل ۳: طبقه‌بندی

- مجموعه داده‌ی MNIST  
- آموزش یک طبقه‌بند دودویی  
- معیارهای ارزیابی عملکرد  
- سنجش دقت با استفاده از اعتبارسنجی متقاطع  
- ماتریس آشفتگی (Confusion Matrix)  
- دقت و بازخوانی (Precision and Recall)  
- موازنه دقت و بازخوانی  
- منحنی ROC  
- طبقه‌بندی چندکلاسه  
- تحلیل خطا  
- طبقه‌بندی چندبرچسبی (Multilabel Classification)  
- طبقه‌بندی چندخروجی (Multioutput Classification)  
- تمرین‌ها  

---

## فصل ۴: آموزش مدل‌ها

- رگرسیون خطی  
- معادله نرمال  
- پیچیدگی محاسباتی  
- گرادیان نزولی  
  - گرادیان نزولی دسته‌ای  
  - گرادیان نزولی تصادفی  
  - گرادیان نزولی دسته‌ای کوچک  
- رگرسیون چندجمله‌ای  
- منحنی‌های یادگیری  
- مدل‌های خطی با منظم‌سازی  
  - رگرسیون ریج (Ridge)  
  - رگرسیون لاسو (Lasso)  
  - رگرسیون شبکه کشسان (Elastic Net)  
- توقف زودهنگام (Early Stopping)  
- رگرسیون لجستیک  
  - تخمین احتمال‌ها  
  - تابع هزینه و آموزش  
  - مرز تصمیم  
- رگرسیون سافت‌مکس  
- تمرین‌ها  

---

## فصل ۵: ماشین‌های بردار پشتیبان (SVM)

- طبقه‌بندی خطی با SVM  
- طبقه‌بندی با حاشیه نرم (Soft Margin)  
- طبقه‌بندی غیرخطی  
  - هسته چندجمله‌ای (Polynomial Kernel)  
  - ویژگی‌های شباهتی  
  - هسته گوسی RBF  
- کلاس‌های SVM و پیچیدگی محاسباتی  
- رگرسیون با SVM  
- پشت‌پرده‌ی طبقه‌بندهای خطی SVM  
  - مسئله‌ی دوگانه (Dual Problem)  
  - SVMهای مبتنی بر هسته  
- تمرین‌ها  

---

## فصل ۶: درخت‌های تصمیم

- آموزش و مصورسازی درخت تصمیم  
- پیش‌بینی با استفاده از درخت  
- برآورد احتمال طبقات  
- الگوریتم آموزش CART  
- پیچیدگی محاسباتی  
- ناخالصی جینی یا آنتروپی؟  
- ابرپارامترهای منظم‌سازی  
- رگرسیون با درخت تصمیم  
- حساسیت به جهت‌گیری ویژگی‌ها  
- واریانس بالا در درخت‌های تصمیم  
- تمرین‌ها  


## فصل ۷: یادگیری تجمیعی و جنگل‌های تصادفی

- طبقه‌بندهای رأی‌گیری (Voting Classifiers)  
- بگینگ و پیستینگ (Bagging & Pasting)  
- پیاده‌سازی بگینگ و پیستینگ در Scikit-Learn  
- ارزیابی خارج از کیسه (Out-of-Bag Evaluation)  
- وصله‌های تصادفی و زیر‌فضاهای تصادفی  
- جنگل‌های تصادفی (Random Forests)  
- درخت‌های اضافی (Extra-Trees)  
- اهمیت ویژگی‌ها (Feature Importance)  
- بوستینگ  
  - آدابوست (AdaBoost)  
  - گرادیان بوستینگ (Gradient Boosting)  
  - گرادیان بوستینگ مبتنی بر هیستوگرام  
- پشته‌سازی (Stacking)  
- تمرین‌ها  

---

## فصل ۸: کاهش ابعاد

- نفرین ابعاد بالا (Curse of Dimensionality)  
- رویکردهای اصلی برای کاهش ابعاد  
  - نگاشت (Projection)  
  - یادگیری منیفولدها (Manifold Learning)  
- تحلیل مؤلفه‌های اصلی (PCA)  
  - حفظ واریانس  
  - مؤلفه‌های اصلی  
  - نگاشت به فضای d بعدی  
  - استفاده از Scikit-Learn  
  - نسبت واریانس توضیح‌داده‌شده  
  - انتخاب تعداد مناسب ابعاد  
  - استفاده از PCA برای فشرده‌سازی  
  - PCA تصادفی  
  - PCA افزایشی  
- نگاشت تصادفی (Random Projection)  
- یادگیری منیفولد به کمک LLE  
- دیگر روش‌های کاهش ابعاد  
- تمرین‌ها  

---

## فصل ۹: روش‌های یادگیری بدون نظارت

- الگوریتم‌های خوشه‌بندی: k-means و DBSCAN  
  - الگوریتم k-means  
  - محدودیت‌های k-means  
  - استفاده از خوشه‌بندی برای قطعه‌بندی تصویر  
  - استفاده از خوشه‌بندی برای یادگیری نیمه‌نظارتی  
  - الگوریتم DBSCAN  
- سایر الگوریتم‌های خوشه‌بندی  
- ترکیب‌های گاوسی (Gaussian Mixtures)  
  - استفاده از مدل‌های گاوسی برای تشخیص ناهنجاری  
  - انتخاب تعداد خوشه‌ها  
  - مدل‌های ترکیبی گاوسی بیزی (Bayesian Gaussian Mixtures)  
- الگوریتم‌های دیگر برای تشخیص ناهنجاری و نوآوری  
- تمرین‌ها  

---

## بخش دوم: شبکه‌های عصبی و یادگیری عمیق

## فصل ۱۰: مقدمه‌ای بر شبکه‌های عصبی مصنوعی با Keras

- از نورون‌های زیستی تا نورون‌های مصنوعی  
  - نورون‌های زیستی  
  - محاسبات منطقی با نورون‌ها  
- پرسپترون  
- پرسپترون چندلایه و الگوریتم پس‌انتشار خطا  
- پرسپترون برای رگرسیون  
- پرسپترون برای طبقه‌بندی  
- پیاده‌سازی پرسپترون چندلایه با Keras  
  - ساخت طبقه‌بند تصویری با API ترتیبی (Sequential API)  
  - ساخت مدل رگرسیون با API ترتیبی  
  - ساخت مدل‌های پیچیده با API تابعی (Functional API)  
  - ساخت مدل‌های پویا با استفاده از Subclassing API  
- ذخیره و بازیابی مدل  
- استفاده از Callbackها  
- مصورسازی با TensorBoard  
- تنظیم ابرپارامترهای شبکه عصبی  
  - تعداد لایه‌های پنهان  
  - تعداد نورون در هر لایه پنهان  
  - نرخ یادگیری، اندازه بچ و سایر ابرپارامترها  
- تمرین‌ها  

---

## فصل ۱۱: آموزش شبکه‌های عصبی عمیق

- مشکلات گرادیان ناپدیدشونده / انفجاری  
- مقداردهی اولیه Glorot و He  
- توابع فعال‌سازی بهتر  
- نرمال‌سازی دسته‌ای (Batch Normalization)  
- برش گرادیان (Gradient Clipping)  
- استفاده مجدد از لایه‌های از پیش آموزش‌دیده  
  - انتقال یادگیری با Keras  
  - پیش‌ آموزش بدون نظارت  
  - پیش‌ آموزش با یک وظیفه کمکی  
- بهینه‌سازهای سریع‌تر  
  - مومنتوم  
  - گرادیان شتاب‌دار نستروف (Nesterov)  
  - آداگراد (AdaGrad)  
  - RMSProp  
  - آدام (Adam)  
  - آدام‌مکس (AdaMax)  
  - نادم (Nadam)  
  - آدام‌دبلیو (AdamW)  
- زمان‌بندی نرخ یادگیری  
- جلوگیری از بیش‌برازش با منظم‌سازی  
  - منظم‌سازی ℓ1 و ℓ2  
  - Dropout  
  - Dropout به روش مونت‌کارلو (MC Dropout)  
  - منظم‌سازی با Max-Norm  
- خلاصه و راهنمایی‌های عملی  
- تمرین‌ها  


## فصل ۱۲: مدل‌های سفارشی و آموزش با TensorFlow

- مرور سریع بر TensorFlow  
- استفاده از TensorFlow مشابه NumPy  
- تنسورها و عملیات‌ها  
- تنسورها و NumPy  
- تبدیل نوع داده‌ها  
- متغیرها  
- سایر ساختارهای داده  
- سفارشی‌سازی مدل‌ها و الگوریتم‌های آموزش  
  - توابع هزینه سفارشی  
  - ذخیره و بارگذاری مدل‌های دارای اجزای سفارشی  
  - توابع فعال‌سازی، مقداردهنده‌ها، منظم‌سازها و قیود سفارشی  
  - معیارهای ارزیابی سفارشی  
  - لایه‌های سفارشی  
  - مدل‌های سفارشی  
  - هزینه‌ها و معیارهای مبتنی بر ساختار داخلی مدل  
- محاسبه گرادیان با استفاده از autodiff  
- حلقه‌های آموزش سفارشی  
- توابع TensorFlow و گراف‌ها  
  - تبدیل خودکار کد به گراف (AutoGraph) و فرآیند ردیابی (Tracing)  
  - قواعد استفاده از tf.function  
- تمرین‌ها  

---

## فصل ۱۳: بارگذاری و پیش‌پردازش داده‌ها با TensorFlow

- رابط کاربری (API) مربوط به `tf.data`
- زنجیره‌سازی تبدیلات  
- درهم‌ریزی (Shuffling) داده‌ها  
- درهم‌آمیختن خطوط از فایل‌های مختلف  
- پیش‌پردازش داده‌ها  
- ترکیب همه مراحل  
- پیش‌بارگذاری (Prefetching)  
- استفاده از Dataset در Keras  
- فرمت TFRecord  
  - فایل‌های TFRecord فشرده‌شده  
- مقدمه‌ای کوتاه بر Protocol Buffers  
  - پروتوباف‌های TensorFlow  
  - بارگذاری و تجزیه نمونه‌ها  
  - مدیریت لیست‌های تو در تو با SequenceExample  
- لایه‌های پیش‌پردازش در Keras  
  - لایه نرمال‌سازی  
  - لایه گسسته‌سازی (Discretization)  
  - لایه کدگذاری دسته‌ای (CategoryEncoding)  
  - لایه StringLookup  
  - لایه Hashing  
  - کدگذاری ویژگی‌های دسته‌ای با استفاده از Embedding  
- پیش‌پردازش متن  
  - استفاده از اجزای از پیش‌آموزش‌دیده برای مدل‌های زبانی  
- لایه‌های پیش‌پردازش تصویر  
- پروژه TensorFlow Datasets  
- تمرین‌ها  

---

## فصل ۱۴: بینایی عمیق کامپیوتری با استفاده از شبکه‌های عصبی کانولوشنی

- معماری قشر بینایی مغز  
- لایه‌های کانولوشنی  
  - فیلترها  
  - پشته‌سازی نگاشت‌های ویژگی متعدد  
  - پیاده‌سازی لایه‌های کانولوشنی با Keras  
  - نیازهای حافظه  
- لایه‌های Pooling  
  - پیاده‌سازی Pooling با Keras  
- معماری‌های CNN  
  - معماری LeNet-5  
  - معماری AlexNet  
  - معماری GoogLeNet  
  - معماری VGGNet  
  - معماری ResNet  
  - معماری Xception  
  - معماری SENet  
  - سایر معماری‌های قابل توجه  
- انتخاب معماری مناسب CNN  
- پیاده‌سازی ResNet-34 با Keras  
- استفاده از مدل‌های از پیش آموزش‌دیده در Keras  
  - مدل‌های از پیش‌آموزش‌دیده برای انتقال یادگیری  
- طبقه‌بندی و مکان‌یابی (Localization)  
- تشخیص اشیاء  
- شبکه‌های کاملاً کانولوشنی  
- الگوریتم YOLO (You Only Look Once)  
- ردیابی اشیاء  
- تقسیم‌بندی معنایی  
- تمرین‌ها  

---

## فصل ۱۵: پردازش دنباله‌ها با استفاده از RNN و CNN

- نورون‌ها و لایه‌های بازگشتی  
- سلول‌های حافظه  
- دنباله‌های ورودی و خروجی  
- آموزش RNNها  
- پیش‌بینی سری‌های زمانی  
  - خانواده مدل‌های ARMA  
- آماده‌سازی داده‌ها برای مدل‌های یادگیری ماشین  
- پیش‌بینی با مدل خطی  
- پیش‌بینی با RNN ساده  
- پیش‌بینی با RNN عمیق  
- پیش‌بینی سری‌های زمانی چندمتغیره  
- پیش‌بینی چند گام جلوتر  
- پیش‌بینی با مدل دنباله به دنباله (Sequence-to-Sequence)  
- مدیریت دنباله‌های بلند  
  - مقابله با مشکل گرادیان‌های ناپایدار  
  - مقابله با مشکل حافظه کوتاه‌مدت  
- تمرین‌ها  


## فصل ۱۶: پردازش زبان طبیعی با RNN و مکانیزم توجه (Attention)

- تولید متن شکسپیری با استفاده از RNN کاراکتری  
  - ایجاد دیتاست آموزشی  
  - ساخت و آموزش مدل Char-RNN  
  - تولید متن شکسپیری ساختگی  
- RNN با حالت حفظ‌شده (Stateful RNN)  
- تحلیل احساسات (Sentiment Analysis)  
- ماسک‌گذاری (Masking)  
- استفاده مجدد از embeddingها و مدل‌های زبانی از پیش‌آموزش‌دیده  
- شبکه Encoder–Decoder برای ترجمه ماشینی عصبی  
- RNN دوطرفه (Bidirectional RNN)  
- جستجوی پرتو (Beam Search)  
- مکانیزم‌های توجه (Attention)  
- «توجه کافی‌ست»: معماری اصلی ترنسفورمر  
- هجوم مدل‌های مبتنی بر ترنسفورمر  
- ترنسفورمرهای بینایی (Vision Transformers)  
- کتابخانه Transformers از Hugging Face  
- تمرین‌ها  

---

## فصل ۱۷: خودرمزگذارها، شبکه‌های مولد تخاصمی (GAN)، و مدل‌های انتشار (Diffusion)

- نمایش‌های داده‌ای کارآمد  
- انجام PCA با خودرمزگذار خطی ناقص (Undercomplete Autoencoder)  
- خودرمزگذارهای پشته‌ای (Stacked Autoencoders)  
  - پیاده‌سازی با Keras  
  - تجسم بازسازی‌ها  
  - تجسم دیتاست Fashion MNIST  
  - پیش‌آموزش بدون نظارت با استفاده از خودرمزگذارهای پشته‌ای  
  - اشتراک وزن‌ها (Tied Weights)  
  - آموزش خودرمزگذار به صورت لایه به لایه  
- خودرمزگذارهای کانولوشنی  
- خودرمزگذارهای حذف نویز (Denoising Autoencoders)  
- خودرمزگذارهای تنک (Sparse Autoencoders)  
- خودرمزگذارهای متغیر (Variational Autoencoders)  
  - تولید تصاویر Fashion MNIST  
- شبکه‌های مولد تخاصمی (GANs)  
  - چالش‌های آموزش GAN  
  - GANهای کانولوشنی عمیق  
  - رشد تدریجی GANها  
  - StyleGAN  
- مدل‌های انتشار (Diffusion Models)  
- تمرین‌ها  

---

## فصل ۱۸: یادگیری تقویتی (Reinforcement Learning)

- یادگیری بهینه‌سازی پاداش  
- جستجوی سیاست (Policy Search)  
- معرفی OpenAI Gym  
- سیاست‌های شبکه عصبی  
- ارزیابی کنش‌ها: مسئله تخصیص اعتبار  
- گرادیان‌های سیاست (Policy Gradients)  
- فرایندهای تصمیم‌گیری مارکوف (MDP)  
- یادگیری تفاوت زمانی (TD Learning)  
- Q-Learning  
- سیاست‌های اکتشافی  
- Q-Learning تقریبی و Q-Learning عمیق  
  - پیاده‌سازی Q-Learning عمیق  
  - انواع مختلف DQN  
    - هدف‌های Q ثابت  
    - Double DQN  
    - بازپخش تجربه با اولویت (Prioritized Experience Replay)  
    - Dueling DQN  
- مروری بر الگوریتم‌های محبوب یادگیری تقویتی  
- تمرین‌ها  

---

## فصل ۱۹: آموزش و استقرار مدل‌های TensorFlow در مقیاس بزرگ

- سرویس‌دهی به یک مدل TensorFlow  
  - استفاده از TensorFlow Serving  
  - ساخت سرویس پیش‌بینی در Vertex AI  
  - اجرای پیش‌بینی‌های دسته‌ای در Vertex AI  
- استقرار مدل در موبایل یا دستگاه‌های تعبیه‌شده  
- اجرای مدل در صفحه وب  
- استفاده از GPU برای تسریع محاسبات  
  - تهیه GPU  
  - مدیریت حافظه GPU  
  - جای‌گذاری عملیات و متغیرها روی دستگاه‌ها  
  - اجرای موازی روی چندین دستگاه  
- آموزش مدل روی چندین دستگاه  
  - موازی‌سازی مدل (Model Parallelism)  
  - موازی‌سازی داده (Data Parallelism)  
- آموزش در مقیاس بزرگ با استفاده از Distribution Strategies  
- آموزش مدل در کلاستر TensorFlow  
- اجرای آموزش‌های بزرگ در Vertex AI  
- تنظیم ابرپارامترها در Vertex AI  
- تمرین‌ها  

---

## ضمیمه A: چک‌لیست پروژه یادگیری ماشین

- تعریف مسئله و درک کلی آن  
- جمع‌آوری داده‌ها  
- اکتشاف داده‌ها  
- آماده‌سازی داده‌ها  
- انتخاب مدل‌های امیدبخش  
- تنظیم دقیق سیستم  
- ارائه راه‌حل  
- راه‌اندازی نهایی  

---

## ضمیمه B: Autodiff

- مشتق‌گیری دستی  
- تقریب با تفاضل محدود  
- autodiff به روش forward-mode  
- autodiff به روش reverse-mode  

---

## ضمیمه C: ساختارهای داده ویژه

- رشته‌ها (Strings)  
- تنسورهای نامنظم (Ragged Tensors)  
- تنسورهای پراکنده (Sparse Tensors)  
- آرایه‌های تنسوری (Tensor Arrays)  
- مجموعه‌ها (Sets)  
- صف‌ها (Queues)  

---

## ضمیمه D: گراف‌های TensorFlow

- توابع `tf.function` و توابع concrete  
- بررسی تعاریف توابع و گراف‌ها  
- نگاهی دقیق‌تر به فرآیند tracing  
- استفاده از AutoGraph برای کنترل جریان  
- مدیریت متغیرها و منابع در توابع TF  
- استفاده یا عدم استفاده از `tf.function` در Keras  

---

## منابع  
## درباره نویسنده  
