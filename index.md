# فهرست مطالب

# مقدمه
- سونامی یادگیری ماشین  
- یادگیری ماشین در پروژه‌های شما  
- هدف و رویکرد  
- پیش‌نیازها  
- نقشه راه  
- تغییرات در نسخه دوم  
- سایر منابع  
- قراردادهای استفاده‌شده در این کتاب  
- نمونه‌های کد  
- نحوه استفاده از نمونه‌های کد  
- یادگیری آنلاین O’Reilly  
- چگونه با ما تماس بگیرید  
- قدردانی  

# بخش اول: اصول پایهٔ یادگیری ماشین

## فصل ۱: چشم‌انداز یادگیری ماشین
- یادگیری ماشین چیست؟  
- چرا از یادگیری ماشین استفاده کنیم؟  
- نمونه‌هایی از کاربردها  
- انواع سامانه‌های یادگیری ماشین  
- یادگیری تحت نظارت / بدون نظارت  
- یادگیری دسته‌ای و یادگیری آنلاین  
- یادگیری مبتنی بر نمونه در برابر یادگیری مبتنی بر مدل  
- چالش‌های اصلی یادگیری ماشین  
  - کمبود داده‌های آموزشی  
  - داده‌های آموزشی غیرنماینده  
  - داده‌های بی‌کیفیت  
  - ویژگی‌های نامربوط  
  - بیش‌برازش روی داده‌های آموزشی  
  - کم‌برازش روی داده‌های آموزشی  
- یک قدم به عقب  
- آزمون و اعتبارسنجی  
- تنظیم ابرپارامترها و انتخاب مدل  
- ناسازگاری بین داده‌ها  
- تمرین‌ها  



## فصل ۲: پروژه‌ی یادگیری ماشین از ابتدا تا انتها
- کار با داده‌های واقعی  
- نگاهی جامع به مسئله  
- تعریف مسئله  
- انتخاب معیار ارزیابی عملکرد  
- بررسی فرضیات  
- دریافت داده  
- ایجاد فضای کاری  
- دانلود داده  
- بررسی سریع ساختار داده  
- ایجاد مجموعه‌ی آزمون  
- کشف و مصورسازی داده برای استخراج بینش  
  - مصورسازی داده‌های جغرافیایی  
  - جستجوی هم‌بستگی‌ها  
  - آزمایش ترکیب ویژگی‌ها  
- آماده‌سازی داده برای الگوریتم‌های یادگیری ماشین  
  - پاک‌سازی داده  
  - مدیریت ویژگی‌های متنی و دسته‌ای  
  - تبدیل‌گرهای سفارشی  
  - مقیاس‌بندی ویژگی‌ها  
  - خطوط پردازش (Pipeline) تبدیل  
- انتخاب و آموزش یک مدل  
  - آموزش و ارزیابی روی مجموعه‌ی آموزش  
  - ارزیابی بهتر با استفاده از اعتبارسنجی متقابل  
- تنظیم دقیق مدل  
  - جستجوی شبکه‌ای (Grid Search)  
  - جستجوی تصادفی (Randomized Search)  
  - روش‌های تجمعی (Ensemble Methods)  
- تحلیل بهترین مدل‌ها و خطاهای آن‌ها  
- ارزیابی نهایی سیستم بر روی مجموعه‌ی آزمون  
- راه‌اندازی، نظارت و نگهداری از سیستم  
- امتحان کنید!  
- تمرین‌ها  

## فصل ۳: دسته‌بندی (Classification)
- مجموعه داده MNIST  
- آموزش یک دسته‌بند دودویی  
- معیارهای ارزیابی عملکرد  
  - اندازه‌گیری دقت با استفاده از اعتبارسنجی متقابل  
  - ماتریس درهم‌ریختگی (Confusion Matrix)  
  - دقت و یادآوری (Precision and Recall)  
  - موازنه بین دقت و یادآوری  
  - منحنی ROC  
- دسته‌بندی چندکلاسه  
- تحلیل خطا  
- دسته‌بندی چندبرچسبی (Multilabel Classification)  
- دسته‌بندی چندخروجی (Multioutput Classification)  
- تمرین‌ها  

## فصل ۴: آموزش مدل‌ها
- رگرسیون خطی  
- معادله نرمال (Normal Equation)  
- پیچیدگی محاسباتی  
- نزول گرادیانی (Gradient Descent)  
  - نزول گرادیان دسته‌ای  
  - نزول گرادیان تصادفی  
  - نزول گرادیان مینی‌بچ  
- رگرسیون چندجمله‌ای  
- منحنی‌های یادگیری  
- مدل‌های خطی منظم‌شده  
  - رگرسیون ریج (Ridge Regression)  
  - رگرسیون لاسو (Lasso Regression)  
  - شبکه الاستیک (Elastic Net)  
- توقف زودهنگام (Early Stopping)  
- رگرسیون لجستیک  
  - تخمین احتمالات  
  - تابع هزینه و آموزش  
  - مرزهای تصمیم‌گیری  
  - رگرسیون Softmax  
- تمرین‌ها  


## فصل ۵: ماشین‌های بردار پشتیبان (SVM)
- دسته‌بندی خطی با SVM  
- دسته‌بندی با حاشیه نرم (Soft Margin)  
- دسته‌بندی غیرخطی با SVM  
  - کرنل چندجمله‌ای  
  - ویژگی‌های شباهت  
  - کرنل تابع پایه شعاعی گاوسی (Gaussian RBF Kernel)  
- پیچیدگی محاسباتی  
- رگرسیون با SVM  
- در زیرساخت  
  - تابع تصمیم و پیش‌بینی‌ها  
  - هدف آموزشی  
  - برنامه‌ریزی درجه دوم (Quadratic Programming)  
  - مسئله دوگان (Dual Problem)  
  - SVM کرنلی‌شده  
  - SVMهای برخط (Online SVMs)  
- تمرین‌ها  

## فصل ۶: درخت‌های تصمیم
- آموزش و مصورسازی یک درخت تصمیم  
- انجام پیش‌بینی  
- برآورد احتمالات دسته‌ها  
- الگوریتم آموزش CART  
- پیچیدگی محاسباتی  
- ناخالصی جینی یا آنتروپی؟  
- ابرپارامترهای منظم‌سازی  
- رگرسیون  
- ناپایداری (Instability)  
- تمرین‌ها  

## فصل ۷: یادگیری تجمعی و جنگل‌های تصادفی
- دسته‌بندهای رأی‌گیری  
- Bagging و Pasting  
- پیاده‌سازی Bagging و Pasting در Scikit-Learn  
- ارزیابی خارج از بسته (Out-of-Bag Evaluation)  
- قطعات تصادفی و زیرفضاهای تصادفی  
- جنگل‌های تصادفی (Random Forests)  
- درخت‌های بسیار تصادفی (Extra-Trees)  
- اهمیت ویژگی‌ها  
- تقویت (Boosting)  
  - آدابوست (AdaBoost)  
  - تقویت گرادیانی (Gradient Boosting)  
- انباشت‌سازی (Stacking)  
- تمرین‌ها  

## فصل ۸: کاهش ابعاد
- نفرین ابعاد (Curse of Dimensionality)  
- رویکردهای اصلی برای کاهش ابعاد  
  - فرافکنی (Projection)  
  - یادگیری منیفلد (Manifold Learning)  
- تحلیل مؤلفه‌های اصلی (PCA)  
  - حفظ واریانس  
  - مؤلفه‌های اصلی  
  - فرافکنی به ابعاد کمتر  
  - استفاده از Scikit-Learn  
  - نسبت واریانس توضیح‌داده‌شده  
  - انتخاب تعداد مناسب ابعاد  
  - استفاده از PCA برای فشرده‌سازی  
  - PCA تصادفی  
  - PCA افزایشی (Incremental PCA)  
  - PCA کرنلی  
  - انتخاب کرنل و تنظیم ابرپارامترها  
- الگوریتم LLE  
- سایر تکنیک‌های کاهش ابعاد  
- تمرین‌ها  

## فصل ۹: تکنیک‌های یادگیری بدون‌نظارت
- خوشه‌بندی  
  - K-Means  
  - محدودیت‌های K-Means  
  - استفاده از خوشه‌بندی برای قطعه‌بندی تصویر  
  - استفاده از خوشه‌بندی در پیش‌پردازش  
  - استفاده از خوشه‌بندی در یادگیری نیمه‌نظارتی  
- DBSCAN  
- سایر الگوریتم‌های خوشه‌بندی  
- مخلوط‌های گاوسی (Gaussian Mixtures)  
  - تشخیص ناهنجاری با استفاده از مخلوط‌های گاوسی  
  - انتخاب تعداد خوشه‌ها  
  - مدل‌های مخلوط گاوسی بیزی  
- سایر الگوریتم‌ها برای تشخیص ناهنجاری و تازگی  
- تمرین‌ها  


## بخش دوم: شبکه‌های عصبی و یادگیری عمیق

### فصل ۱۰: مقدمه‌ای بر شبکه‌های عصبی مصنوعی با Keras
- از نورون‌های زیستی تا نورون‌های مصنوعی  
- نورون‌های زیستی  
- محاسبات منطقی با نورون‌ها  
- پرسپترون  
- پرسپترون چندلایه و الگوریتم پس‌انتشار  
- MLP برای رگرسیون  
- MLP برای دسته‌بندی  
- پیاده‌سازی MLP با Keras  
- نصب TensorFlow 2  
- ساخت یک دسته‌بند تصویر با API ترتیبی (Sequential API)  
- ساخت MLP برای رگرسیون با API ترتیبی  
- ساخت مدل‌های پیچیده با API تابعی (Functional API)  
- ساخت مدل‌های پویا با API زیرکلاسبندی (Subclassing API)  
- ذخیره‌سازی و بازیابی مدل  
- استفاده از Callbackها  
- استفاده از TensorBoard برای مصورسازی  
- تنظیم دقیق ابرپارامترهای شبکه عصبی  
  - تعداد لایه‌های پنهان  
  - تعداد نورون‌ها در هر لایه پنهان  
  - نرخ یادگیری، اندازه دسته، و سایر ابرپارامترها  
- تمرین‌ها  

### فصل ۱۱: آموزش شبکه‌های عصبی عمیق
- مسئله ناپدید شدن/منفجر شدن گرادیان‌ها  
- مقداردهی اولیه Glorot و He  
- توابع فعال‌سازی غیر اشباع  
- نرمال‌سازی دسته‌ای (Batch Normalization)  
- برش گرادیان (Gradient Clipping)  
- استفاده مجدد از لایه‌های پیش‌آموخته  
- یادگیری انتقالی با Keras  
- پیش‌آموزش بدون‌نظارت  
- پیش‌آموزی بر اساس یک وظیفه کمکی  
- بهینه‌سازهای سریع‌تر  
  - بهینه‌سازی با مومنتوم  
  - گرادیان شتاب‌گرفته نستروف (Nesterov)  
  - AdaGrad  
  - RMSProp  
  - Adam و Nadam  
- زمان‌بندی نرخ یادگیری  
- جلوگیری از بیش‌برازش (Overfitting) از طریق منظم‌سازی  
  - منظم‌سازی ℓ1 و ℓ2  
  - Dropout  
  - Dropout به روش مونت‌کارلو (MC Dropout)  
  - منظم‌سازی Max-Norm  
- خلاصه و راهنمای عملی  
- تمرین‌ها  

### فصل ۱۲: مدل‌ها و آموزش‌های سفارشی با TensorFlow
- مرور سریع TensorFlow  
  - استفاده از TensorFlow مشابه NumPy  
  - تنسورها و عملیات‌ها  
  - تبدیل بین تنسور و NumPy  
  - تبدیل نوع‌ها  
  - متغیرها  
  - ساختارهای داده دیگر  
- سفارشی‌سازی مدل‌ها و الگوریتم‌های آموزش  
  - توابع زیان سفارشی  
  - ذخیره و بارگذاری مدل‌هایی با مؤلفه‌های سفارشی  
  - توابع فعال‌سازی، مقداردهی اولیه، منظم‌سازها و محدودکننده‌های سفارشی  
  - معیارهای سفارشی  
  - لایه‌های سفارشی  
  - مدل‌های سفارشی  
  - زیان‌ها و معیارهایی مبتنی بر اجزای داخلی مدل  
- محاسبه گرادیان با Autodiff  
- حلقه‌های آموزش سفارشی  
- توابع و گراف‌های TensorFlow  
  - AutoGraph و رهگیری  
  - قوانین تابع TF  
- تمرین‌ها  

### فصل ۱۳: بارگذاری و پیش‌پردازش داده با TensorFlow
- API داده  
  - زنجیره‌سازی تبدیلات  
  - درهم‌ریزی داده  
  - پیش‌پردازش داده  
  - یکپارچه‌سازی همه موارد  
  - پیش‌واکشی (Prefetching)  
  - استفاده از Dataset با tf.keras  
- فرمت TFRecord  
  - فایل‌های فشرده‌شده TFRecord  
  - مقدمه‌ای بر پروتکل بافرها (Protocol Buffers)  
  - Protobufهای TensorFlow  
  - بارگذاری و تجزیه نمونه‌ها  
  - مدیریت لیست‌های تو در تو با SequenceExample Protobuf  
- پیش‌پردازش ویژگی‌های ورودی  
  - کدگذاری ویژگی‌های دسته‌ای با بردارهای One-Hot  
  - کدگذاری ویژگی‌های دسته‌ای با جاسازی (Embedding)  
- لایه‌های پیش‌پردازش Keras  
- ابزار TF Transform  
- پروژه داده‌های TensorFlow (TFDS)  
- تمرین‌ها  


## فصل ۱۴: بینایی ماشین عمیق با استفاده از شبکه‌های عصبی کانولوشنی (CNN)

- معماری قشر بینایی مغز
- لایه‌های کانولوشنی
- فیلترها
- انباشت چند نقشه ویژگی (Feature Map)
- پیاده‌سازی در TensorFlow
- نیازهای حافظه
- لایه‌های Pooling
- پیاده‌سازی در TensorFlow
- معماری‌های معروف CNN
  - LeNet-5
  - AlexNet
  - GoogLeNet
  - VGGNet
  - ResNet
  - Xception
  - SENet
- پیاده‌سازی CNN مبتنی بر ResNet-34 با Keras
- استفاده از مدل‌های پیش‌آموخته Keras
- یادگیری انتقالی با مدل‌های پیش‌آموخته
- طبقه‌بندی و مکان‌یابی (Classification and Localization)
- تشخیص اشیاء (Object Detection)
- شبکه‌های کاملاً کانولوشنی (Fully Convolutional Networks)
- الگوریتم YOLO (You Only Look Once)
- تقسیم‌بندی معنایی (Semantic Segmentation)
- تمرین‌ها

## فصل ۱۵: پردازش دنباله‌ها با استفاده از RNN و CNN

- نورون‌ها و لایه‌های بازگشتی (Recurrent)
- سلول‌های حافظه (Memory Cells)
- دنباله‌های ورودی و خروجی
- آموزش RNNها
- پیش‌بینی سری‌های زمانی
- معیارهای پایه (Baseline Metrics)
- پیاده‌سازی RNN ساده
- RNNهای عمیق
- پیش‌بینی چند گام آینده
- مدیریت دنباله‌های طولانی
- مقابله با مشکل گرادیان‌های ناپایدار
- حل مشکل حافظه کوتاه‌مدت
- تمرین‌ها

## فصل ۱۶: پردازش زبان طبیعی با RNN و مکانیزم توجه (Attention)

- تولید متن شکسپیری با RNN نویسه‌ای (Character RNN)
- ساخت دیتاست آموزشی
- نحوه تقسیم دیتاست‌های دنباله‌ای
- تقسیم دیتاست به پنجره‌های متعدد
- ساخت و آموزش مدل Char-RNN
- استفاده از مدل Char-RNN
- تولید متن شبیه شکسپیر
- RNNهای دارای وضعیت (Stateful RNN)
- تحلیل احساسات (Sentiment Analysis)
- Masking (پنهان‌سازی مقادیر بی‌اهمیت)
- استفاده مجدد از جاسازی‌های پیش‌آموخته (Embeddings)
- شبکه Encoder–Decoder برای ترجمه ماشینی عصبی
- RNNهای دوطرفه (Bidirectional RNNs)
- جست‌وجوی پرتوی (Beam Search)
- مکانیزم‌های توجه (Attention Mechanisms)
- توجه بصری (Visual Attention)
- "توجه، تمام آن چیزی است که نیاز دارید": معماری Transformer
- نوآوری‌های اخیر در مدل‌های زبانی
- تمرین‌ها

## فصل ۱۷: یادگیری بازنمایی و یادگیری مولد با استفاده از Autoencoder و GAN

- بازنمایی‌های کارآمد از داده‌ها
- اجرای PCA با Autoencoder خطی و ناقص (Undercomplete)
- Autoencoderهای پشته‌ای (Stacked Autoencoders)
- پیاده‌سازی Autoencoder پشته‌ای با Keras
- مصورسازی بازسازی‌ها
- مصورسازی دیتاست Fashion MNIST
- پیش‌آموزش بدون‌نظارت با Autoencoderهای پشته‌ای
- اشتراک وزن‌ها (Tied Weights)
- آموزش Autoencoderها به‌صورت مجزا
- Autoencoderهای کانولوشنی
- Autoencoderهای بازگشتی
- Autoencoderهای نویززدا (Denoising)
- Autoencoderهای تنک (Sparse)
- Autoencoderهای واریاسیونی (VAE)
- تولید تصاویر Fashion MNIST
- شبکه‌های مولد تقابلی (GANs)
- چالش‌های آموزش GAN
- GANهای کانولوشنی عمیق (DCGAN)
- رشد تدریجی GANها
- StyleGANها
- تمرین‌ها


## فصل ۱۸: یادگیری تقویتی (Reinforcement Learning)

- یادگیری برای بهینه‌سازی پاداش‌ها
- جست‌وجوی سیاست (Policy Search)
- معرفی OpenAI Gym
- سیاست‌های مبتنی بر شبکه عصبی
- ارزیابی اعمال: مسئله تخصیص اعتبار (Credit Assignment Problem)
- گرادیان سیاست (Policy Gradients)
- فرایندهای تصمیم‌گیری مارکوف (Markov Decision Processes)
- یادگیری اختلاف زمانی (Temporal Difference Learning)
- Q-Learning
- سیاست‌های کاوشی (Exploration Policies)
- Q-Learning تقریبی و Q-Learning عمیق
- پیاده‌سازی Q-Learning عمیق
- انواع Q-Learning عمیق
  - اهداف ثابت Q-Value
  - Double DQN
  - بازپخش تجربه اولویت‌دار (Prioritized Experience Replay)
  - Dueling DQN
- کتابخانه TF-Agents
  - نصب TF-Agents
  - محیط‌های TF-Agents
  - مشخصات محیط
  - پوشش‌دهنده‌های محیط و پیش‌پردازش Atari
- معماری آموزش
  - ساخت شبکه عصبی Q
  - ساخت عامل DQN
  - ساخت حافظه بازپخش و مشاهده‌گر مربوطه
  - ساخت معیارهای آموزش
  - ساخت Collect Driver
  - ساخت دیتاست
  - ساخت حلقه آموزشی
- مروری بر برخی از الگوریتم‌های مشهور RL
- تمرین‌ها

## فصل ۱۹: آموزش و استقرار مدل‌های TensorFlow در مقیاس بزرگ

- ارائه (Serving) مدل‌های TensorFlow
- استفاده از TensorFlow Serving
- ساخت سرویس پیش‌بینی در پلتفرم GCP AI
- استفاده از سرویس پیش‌بینی
- استقرار مدل بر روی دستگاه‌های موبایل یا تعبیه‌شده
- استفاده از GPU برای تسریع محاسبات
  - تهیه GPU شخصی
  - استفاده از ماشین مجازی دارای GPU
  - Google Colab
- مدیریت RAM در GPU
- تخصیص عملیات و متغیرها به دستگاه‌ها
- اجرای موازی در چند دستگاه
- آموزش مدل در چند دستگاه
  - موازی‌سازی مدل
  - موازی‌سازی داده
- آموزش در مقیاس با استفاده از Distribution Strategies API
- آموزش مدل روی کلاستر TensorFlow
- اجرای آموزش‌های بزرگ‌مقیاس در پلتفرم GCP AI
- تنظیم ابرپارامتر به‌صورت جعبه‌سیاه (Black Box) روی AI Platform
- تمرین‌ها

## سپاس‌گزاری

---

## پیوست A: پاسخ تمرین‌ها

- فصل ۱: چشم‌انداز یادگیری ماشین
- فصل ۲: پروژه‌ی یادگیری ماشین انتها به انتها
- فصل ۳: طبقه‌بندی
- فصل ۴: آموزش مدل‌ها
- فصل ۵: ماشین بردار پشتیبان (SVM)
- فصل ۶: درخت‌های تصمیم‌گیری
- فصل ۷: یادگیری تجمیعی و جنگل تصادفی
- فصل ۸: کاهش ابعاد
- فصل ۹: روش‌های یادگیری بدون نظارت
- فصل ۱۰: مقدمه‌ای بر شبکه‌های عصبی مصنوعی با Keras
- فصل ۱۱: آموزش شبکه‌های عصبی عمیق
- فصل ۱۲: مدل‌ها و آموزش‌های سفارشی با TensorFlow
- فصل ۱۳: بارگذاری و پیش‌پردازش داده با TensorFlow
- فصل ۱۴: بینایی ماشین عمیق با CNN
- فصل ۱۵: پردازش دنباله‌ها با RNN و CNN
- فصل ۱۶: پردازش زبان طبیعی با RNN و توجه
- فصل ۱۷: یادگیری بازنمایی و مولد با Autoencoder و GAN
- فصل ۱۸: یادگیری تقویتی
- فصل ۱۹: آموزش و استقرار مدل‌ها در مقیاس بزرگ

## پیوست B: چک‌لیست پروژه یادگیری ماشین

- تعریف مسئله و درک کلی
- تهیه داده
- کاوش داده
- آماده‌سازی داده
- انتخاب مدل‌های مناسب
- تنظیم نهایی سیستم
- ارائه راه‌حل
- استقرار نهایی

## پیوست C: مسئله دوگان SVM

## پیوست D: مشتق‌گیری خودکار (Autodiff)

- مشتق‌گیری دستی
- تقریب مشتق با اختلاف متناهی
- Autodiff به روش Forward Mode
- Autodiff به روش Reverse Mode

## پیوست E: سایر معماری‌های محبوب شبکه‌های عصبی مصنوعی

- شبکه‌های Hopfield
- ماشین‌های Boltzmann
- ماشین‌های Boltzmann محدود (RBM)
- شبکه‌های باور عمیق (DBN)
- نقشه‌های خودسازمان‌ده (Self-Organizing Maps)

## پیوست F: ساختارهای داده‌ای خاص

- رشته‌ها (Strings)
- Tensorهای ناهمگن (Ragged Tensors)
- Tensorهای تنک (Sparse Tensors)
- آرایه‌های Tensor
- مجموعه‌ها (Sets)
- صف‌ها (Queues)

## پیوست G: گراف‌های TensorFlow

- توابع TF و توابع Concrete
- بررسی تعاریف تابع و گراف
- نگاهی دقیق به Tracing
- استفاده از AutoGraph برای کنترل جریان
- مدیریت متغیرها و منابع در توابع TF
- استفاده از TF Functions با tf.keras (یا بدون آن)
